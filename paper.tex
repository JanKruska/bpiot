% !TeX spellcheck = en_GB
% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{template/llncs}
%
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage[autostyle]{csquotes}  
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Conformance Checking Using Embeddings and their Applicability in the Internet of Things}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Jan Kruska}
%
\authorrunning{Jan Kruska}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Chair of Process and Data Science\\Department of Computer Science, RWTH Aachen, Aachen, Germany}
%	\email{jan.kruska@rwth-aachen.de}\\
%	\url{http://www.pads.rwth-aachen.de/}}
%
\maketitle              % typeset the header of the contribution
%
%
%
%
%\section{Planned structure}
%The following list shall give a short overview of the intended general structure of the seminar paper. For each section the main questions that should be answered by it are listed.
%\begin{enumerate}
%	\item Abstract
%	\item Introduction
%	\begin{enumerate}
%		\item What is the motivation of the paper
%	\end{enumerate}
%	\item Background
%	\begin{enumerate}
%		\item What is conformance checking, how is it useful and why would you do it, which methods are there, what are advantages and disadvantages of said methods.
%		\item What is NLP (briefly), what are embeddings generally and word embeddings specifically, what are word2vec and doc2vec and how do they approach the NLP problems.
%		\item What parallels are there between NLP and conformance checking
%		\item If there are these parallels, how could such NLP mechanism be adapted (theoretically not practically) for conformance checking, what could the advantages be of such an approach, what could be disadvantages and/or problems that will need addressing.
%		\item What is the Internet of Things and why would one wish to perform process analysis there
%	\end{enumerate}
%	\item Method
%	\begin{enumerate}
%		\item How were word2vec and doc2vec adapted to conformance checking in the paper (I.e. practical adaption)
%	\end{enumerate}
%	\item Results
%	\begin{enumerate}
%		\item What are the results given in the paper
%		\item What metrics were used to evaluate the performance of the approach presented in the paper, are they appropriate, are there other possible metrics, if so why was this one chosen, what advantages or disadvantages do different metrics have.
%		\item What are the results when using their code and reproducing the results (ideally the same)
%	\end{enumerate}
%	\item Outlook
%	\begin{enumerate}
%		\item What future work do the authors suggest
%		\item What problems were encountered when trying to reproduce the results from the paper. Could they be reproduced completely, just in part or not at all? If so, are there suggestions on how could these problems be mitigated.
%	\end{enumerate}
%\end{enumerate}

\begin{abstract}
	In the last decade there has been growing interest, both scientific and non-scientific, in the field of process mining.
	While the main focus has been on business processes, the methods developed in the field of process mining are in now way limited to that area.
	One other area which lends itself to process mining is the growing Internet of Things.
	While there are many conceivable applications,  there has only been limited research interest in this synergy.
	The aim of this seminar paper is to recapitulate the findings of the focus paper \cite{PBWe20}, while also emphasizing the benefits and difficulties of applying the described methodology in the Internet of Things.
\end{abstract}

\section{Introduction}

Due to growing compute capabilities, as well as further digitalization allowed for more process data to be gathered and analysed.
The increase in compute and storage capabilities, as well as the growing digitalization of many parts of our lives has created the distinct field of data science, as an amalgamation of Statistics, technical knowledge and domain knowledge.

One benefactor of this development has been the subfield of process science, which is concerned with the analysis of processes.
The combination of process science and data science gave rise to techniques generally grouped under the term process mining\footnote{Please note that due to the relative young age of the field, the fact that it lies at the intersection of two different fields and the fact that there is a marketing interest for these topics the terminology is not always settled and different authors and/or groups may differ in usage. The terminology used here is oriented around the definitions in \cite{Aals16}, as it seems to be the most polished overview of the field at the moment.}.
Process mining is concerned with the automatic and algorithmic processing of event data. In its easiest form an event is a piece of data, that consists of a timestamp, an identifier to group events belonging to the same execution and a body containing further arbitrary values.
In most cases there is also an identifier for the event type, with the purpose of identifying similar events in different executions of the process, e.g. to be able to group all "Send invoice" events as the same type.

Historically most research in the topic of process mining has been on business processes. 
There are a few reasons for that. 
Namely, that there was already a well established field of non-algorithmic process science for business processes, which could act as a stepping stone.
Secondly some of these processes were already highly digitized, many companies already had well established ERP systems from which data could be gathered with reasonable ease.
Thirdly there was, and is, a large financial interest in optimizing such processes.
And lastly business processes often struck the balance between too simple and too complicated, in that they exhibited enough variability to be interesting to analyse but were still simple enough as to enable the use of the early process mining algorithms.
So it should be stated that while process mining is of great use for classical business processes, these are not the only areas for which it could potentially be useful.

In the last two decades the cost for networking components has decreases significantly and a growing number of microcontrollers, embedded systems and electronic devices now have some form of networking ability.
This increasing networked structure that enables easy information sharing between the members of the network has become known under a few names, most prominently sa the "Internet of Things".
Unfortunately due to shifting developments in the last two decades and marketing efforts associated with the Internet of Things it is hard to pin down the exact meaning of the term. 
The International Telecommunication Union gives the following succinct definition of the Internet of Things in their recommendation \cite{ITUT12} regarding just that:
\blockquote{[The Internet of Things is] A  global  infrastructure  for  the  information  society,  enabling  advanced  services  by  interconnecting  (physical  and  virtual)  things  based  on  existing  and  evolving  interoperable information and communication technologies.}

It should be noted that this definition is very broad and extends beyond the notion of the "Smart Home" that has become somewhat of a representative of the Internet of Things.
The definition neither limits the scale (locally, nationally or even globally) nor the range of use cases.
The rest of the paper will follow this definition since the object of interest here are fully digitized networked control systems and the information pooling and sharing those bring with it.


This vast infrastructure of small networked devices could prove to be fertile ground for process analysis.
These controllers generally have some attached sensors to monitor and control the devices they are attached to and in combination with these produce an equally vast amount of events.
This is especially interesting in the case of automation in manufacturing\cite{JKM*20} for which the term Internet of Production has been coined, here the hope is that networked information sharing between controllers on a production line or factory floor could greatly enhance the production capabilities of not just single production lines but entire automated factories.
Especially in the case of manufacturing there is a large financial interest in optimizing production efficiency for various reasons such as increasing speed, increasing productivity or reducing downtime.

However, the Internet of Things is also very attractive from the process analysis research perspective.
During the birth of process mining as a discipline a big focus laid on classical business processes, such as purchase-to-pay and order-to-cash.
Yet as the field matures the capabilities of algorithms and accordingly the difficulty of interesting problems grows.
As such there is research demand \cite{JKM*20} for different -- to test whether algorithms perform equally well in other scenarios --, larger -- to improve scalability of algorithms -- and more complex -- to test and improve the capacity to deal with more interwoven processes -- problems.
At the same time only areas with a high level of digitalization can realistically be considered here since it is of paramount priority to be able to properly obtain the event data of any process in question.
This has been a problem in the past when considering any process for which event data has to be manually entered, since in all these cases there will be discrepancies between digitally recorded process and the real world execution.
The Internet of Things fulfils all these categories, it is digital by design, so relatively few efforts have to be made to obtain large amount of well-enough structured event data and the scale and complexity of the problem can scale up very well with growing network sizes. 
Furthermore it is also possible to use techniques from the Internet of Things to further digitize classical processes and reduce dependencies on manual event recording.

Naturally there are challenges to be overcome if one aims to integrate business process modelling into the Internet of Things.
The first two major difference are attributable to the structure of the Internet of Things.
Firstly the Internet of Things is made up numerous small devices all with limited compute capabilities.

\section{Background}
\subsection{Conformance Checking}
Conformance checking covers a range of different methods that aim to quantify how well an event log (or even just a single trace) and a process model fit together.
Our intuitive expectation would be that a model that allows for exactly those traces contained in the event log would conform perfectly, whereas a model that allows for none of the traces seen in the log would have very low conformance.
The aim of conformance checking is the formalization of this imprecise intuitive understanding of fitting together and the development of methods and algorithms that can accurately quantify precisely this intuition.

\subsubsection{Uses}
The uses of such an approach can generally be divided into two relatively distinct subcategories, depending on whether the event log or the process model are seen as normative.
If the process model is considered normative, conformance checking allows for the detection of deviations, i.e. observed traces that are not possible according to the model.
From such information a number of useful analyses can follow, e.g. where do deviations happen, which resources are involved in deviations, and how exactly these traces deviate from the model.
It should be pointed out that the word "deviation" is negatively connotated in English, however for the purpose of process intelligence it should be viewed as a neutral category. 
For example an analysis may reveal that people often skip a step in the process, which actually improves the overall performance of the process, based on which refinements to the process model could be made.
An example of negative conclusions may be the detections of deviations that do not follow proper legal or procedural requirements of the process, such as requiring two different persons to review an important document before signing off on it. In such a case knowledge of these deviations is required to ensure proper execution of the process.

In the second case the log is seen as normative and the model considered either imperfect or unknown.
The most important representative application of this perspective is process discovery, or more specifically the evaluation of discovery algorithms.
In the case of process discovery a process model has to be constructed from an event log.
Obviously not all process models are able to describe a process equally good.
So when developing or using a process discovery algorithm the question how well the discovered process models describe the log often arises.

\subsubsection{Quality criteria}
It is important to note that model quality is not a single dimensional criterion, instead it exists in a multi-dimensional space where the different axes represent different quality criteria.
As such it is not really appropriate to talk about one conformance metric, but instead the four major axes should be considered.
1. Precision (false negatives): The first of these metrics is precision. This measures how many of the observed traces are actually possible according to the model.
2. Recall (false positives): Sometimes also referred to as sensitivity, this metric measures how many false positives there are. In the case of process models, this is the fraction of traces allowed by the model that are actually observed in the log.
3. Simplicity: How "simple" a model is, generally a smaller model that describes a process equally well is preferred for reasons such as performance and comprehensibility .
4. Generalization: Generalization is the models ability to be applicable to possible future traces of a given process. 
It would clearly be possible to create a model with perfect precision and recall, by simply encoding the traces themselves in the model. However that would not be a model that generalizes well since it captures none of the dynamism of the underlying process. 
This is the most difficult criterion, since it is arguing about unknown possible future behaviour

Of these four quality criteria conformance checking is mainly concerned precision and recall, but depending on the algorithms possibly some statements about generalization could be made as well. 

\subsubsection{Token based Replay}

The easiest conformance checking method, both conceptually and algorithmically is probably Token based replay.
The intuition behind token based replay is to replay an observed trace on a process model, specifically a petri net, and to determine where problems occur.

This is done by keeping track of four numbers: \emph{\underline{p}roduced}, \emph{\underline{c}onsumed}, \emph{\underline{m}issing} and \emph{\underline{r}emaining} tokens during replay.
According to the semantics of petri nets, whenever a transition is fired it consumes one token from all input places and produces one token in each output place.
However since we are dealing with possibly non-conformant trace, it is possible that a trace contains activities, that are not enabled in the model at that point in time.
In this case a missing token is added to all input places, that did not contain a token, and the $m$ counter is incremented by the according amount.
Conversely, it might also happen, that a trace can be replayed, but that there are stray tokens left in the petri net after the replay. 
At the end of the replay the sum of all leftover tokens makes up the $r$ counter. \todo{Some explanation models could be included if the space is there for them}
Both missing and remaining tokens signal deviation from the process model, but both of these numbers are dependent on the length of traces.
The longer a trace is the more activities will be taken, the more tokens will be produced, and the more chances there are for tokens to be missing or remaining.
Importantly a trace where half the produced tokens were never consumed, i.e. remained, should intuitively have a lower fitness than one where only a quarter of produced tokens remained.
Of course the same relationship holds for the other two counters $m$ and $c$.
Then one very naturally arrives at a trace fitness defined as:
\begin{equation}
fitness(\sigma,N) = \frac{1}{2}(1-\frac{m_{N,\sigma}}{c_{N,\sigma}})+\frac{1}{2}(1-\frac{r_{N,\sigma}}{p_{N,\sigma}})
\end{equation}
, where $x_{N,\sigma}$ is the counter when replaying trace $\sigma$ on petri net $N$.
Often one does not just want to determine the fitness of one trace but a complete log, then this definition can be extended to 
\begin{equation}
	fitness(L,N) = \frac{1}{2}(1-\frac{\sum_{\sigma\in L}m_{N,\sigma}}{\sum_{\sigma\in L}c_{N,\sigma}})+\frac{1}{2}(1-\frac{\sum_{\sigma\in L}r_{N,\sigma}}{\sum_{\sigma\in L}p_{N,\sigma}})
\end{equation}
Note that log-fitness is not mathematically equivalent to the mean trace fitness.

Token based methods are generally easy to implement and efficient algorithms are known, however they are not without problems.
The first is the phenomenon known as "token flooding". Since token based methods create missing tokens to enable observed transitions in the log and remaining tokens are not removed, as more missing tokens are added during the simulation of a trace the  amount of stray tokens in the net also grows.
This overabundance of tokens in the net can then lead to situations were many more transitions are enabled than should be, and later deviations are not detected by token based replay, since they are masked by earlier deviations.
Also, token based diagnostics are  by their nature place-based diagnostics, they can determine whether a place was missing a token for the execution of a trace, but they cannot determine why said place was missing a token. 
Yet in conformance checking it is often the latter rather than the former question for which answers are desired. 

\subsubsection{Alignments}

The second popular method for conformance checking are alignment based methods.
The goal here is to construct an execution on the model that is as "close" as possible to the observed execution of events in the trace/log.
If a trace is conformant with the model, this is trivially easy, the "closest" execution possible in the model is exactly the observed trace.
In the conformant case all moves are so called, synchronous moves, where both the log and the model performed the same event at each step.
However, if a trace is non-conformant, which is of course the case that is of interest, then a one-to-one mapping between model and trace is not possible.
To describe that so-called non-synchronous moves are introduced, these describe the case that one of  the model or log perform an activity that does not happen in the other one.
This can be either a move-on-model-only, where the model takes some transition, which was not observed in the log, or its dual, the move-on-log-only where an event is recorded in the log that is not possible according to the model.
The weighted sum of misalignments can then be used as a cost metric for an alignment. 
The goal is then to construct an optimal alignment between a trace and the model.
It should be obvious that there are infinitely many such alignments, however we can construct a trace-agnostic alignment, which consists of $n$ moves-on-log-only followed by the shortest path through the model as moves-on-model-only.
This means there is an effective upper limit on the length of alignments that need to be considered, which in turn means that the problem is decidable. 
However, the search space can quickly grow infeasibly large, so heuristics need to be employed to determine optimal alignments in a reasonable time frame.

\todo{formula}
\todo{Possibly some examples}

Alignments overcome many of the problems encountered in token based approaches: 
They do not suffer from problems like token flooding, and they give transitions based diagnostics, which in the case of process mining is far more useful since transitions encode recorded or recordable activities. 
As such in alignments it is much easier to tell if a certain activity was skipped in a trace without performing further refinements of the diagnostics.
These benefits do not come without drawbacks though, the main caveat of alignment based methods is that it boils down to a complex search in the alignment space, which is far more demanding than a replay based method.
As such much research interest has been devoted to the development of heuristics that exploit certain  structures in event logs and/or process models to boost the performance of this search.

\subsection{Natural Language Processing}

As laid out in the previous section, both classical branches of conformance checking exhibit major drawbacks.
Besides approaches aiming to ameliorate certain disadvantages of such methods, there has also been a growing interest in applying novel techniques from other fields to the problem of conformance checking.
One such proposal is the application of techniques from the area of natural language processing for conformance checking as discussed in \cite{PBWe20}.

\subsection{Embeddings}
Embeddings are a popular machine learning approach that has been used for a variety of tasks with the most successful applications being in the fields of natural language processing and computer vision.
The fundamental idea of embeddings is to train some form of embedding function that embeds complex objects, such as words or documents in the case of NLP or pictures and video sequences in the case of computer vision in a vectors space\footnote{To be more precise: a Hilbert space, since well-defined notions of distances and angles are necessary and for that a scalar product  is needed.}.
Such an embedding function then enables a proper mathematical treatment of these non-vector objects as vectors, which allows for the application of a broad range of algorithms.
Most importantly it makes it possible to define notions of distance and/or (dis-)similarity needed for machine learning and data analysis tasks.

\subsection{word2vec}

\section{Method}

A lot of conformance checking algorithms, notably the classical ones outlined above aim to measure conformance between a log and a model.
While this is the fundamental question to be answered this comparison between two different objects greatly complicates the task.
Instead framing the problem in terms of log-to-log comparisons can make the problem much easier to deal with, and allows for a much broader array of techniques, since a multitude of approaches for comparison of similar objects exists.
Such a reformulation of the problem can be achieved since all (reasonably defined) models are able to generate sample traces, and can therefore produce a log.
This has the additional advantage that it is model-independent, i.e. no limitations except for the ability to generate traces are put on the type of model.

First an activity embeddings for the log were trained, which then allows for the comparison of two traces by comparing their individual activities in the embedding space.
Given some similarity metric and the real log as well as the model log a dissimilarity matrix can be built up.
From here on we will assume that each row corresponds to a trace from the model log and each column corresponds to a trace from the real log, i.e. $a_{ij}$ is the dissimilarity of the $i$-th trace from the model log with the $j$-th trace of the real log.
As such each column contains the dissimilarity of a trace in the real log with all traces from the model log, with the minimum of a column being the closest match in the model log.
A trace that can be closely replayed by the model, i.e. one that has a close match to a trace produced from the model, is a well fitting one.
As such the column minimum corresponds to a fitness metric of the trace and thus the mean of all column minima is a fitness metric between log and log.
Conversely, each row contains all similarities between a trace in the model log and all traces in the real log.
A precise model should not allow for more traces than observed in the real log, as such each model trace should have as close a match as possible in the real log for the model to be precise.
As such the row minimum corresponds to a precision metric for the trace in the model log and thus the mean of all row minima is a precision metric between log and log.

\subsection{Distance functions}
\subsubsection{Word Mover's Distance}

\subsubsection{Trace embeddings}
In contrast to the two previous approaches, which used activity embeddings, it is also possible to train trace embeddings.
The relationship between \emph{act2vec} and \emph{trace2vec} is analogous to the one between \emph{word2vec} and \emph{doc2vec}.
This introduces additional instability on the trace level, since there will generally many more traces than activities, but allows for the direct application of distance metrics in the vector space when comparing two traces.
This is much less computationally expensive as the previous two approaches which were complex distance metric between two sets of embeddings.

\section{Results}

\subsection{Experimental setup}

To evaluate the suitability of the method three sets of experiments were performed. 
The first set of experiments focussed on the scalability of the approach, especially since more traditional approaches do not scale well. This aspect is doubly important when talking about the Internet of Things, where computing resources are generally far more constrained, while still producing large amounts of event data.

The second set of experiments aimed to determine whether the method responded as expected when providing noisy logs.
These experiments were designed to verify the soundness of approach.
This was done by generating logs from random process trees generated by pm4py, applying random noise on these logs and determining the conformance between the noiseless and noisy logs.
The expectation is that any reasonable conformance metric should deteriorate the more noise is added to the log.

The third and last set of experiments was designed to compare the performance of the proposed methods (WMD,ICT \& trace2vec) to existing established conformance metrics, namely alignments, behavioural fitness \cite{GMVB09} and ETC precision \cite{MuCa10}.
This was achieved by generating 1000 traces from a randomly generated process tree, which form the real log.
Then a discovery algorithm was applied to this log to discover a model, here Alpha Miner, Inductive Miner infrequent (IMi) with noise levels 0 and 1 and the ILP Miner were selected as discovery algorithms.
Then fitness and precision between the discovered model and the real log were calculated using the previously mentioned algorithms as well as the method described in the paper.

\section{Discussion}

Peeperkorn et al. obtained promising first results for a novel bedding based approach towards conformance checking.
Yet as the focus was mainly on the introduction and development of a prototype for such an approach there are some natural limitations.
Arguably the biggest limitation are the logs used for the experimental validation.
All used logs were completely synthetic, generated from random process trees.
It is unclear how well the 12 trees used for this resemble real processes and whether these tree configurations are sufficiently representative for the variation exhibited in real life trees\footnote{12 tree configurations were used, 3 sizes and 4 different probability distributions for operations, how strong these vary should be investigated}.
As such it should be a priority to test the proposed methods on real logs, possibly popular benchmark cases and also to systematically broaden the parameters for random process tree generation to cover more different classes.

%Secondly the sizes of the process logs for the scalability experiment were between 100 to 5000 traces, the log sizes for the noise and discovery experiment were fixed at 1000 traces.

Furthermore, the algorithms used have several parameters which were set as seemingly reasonable standard values.
A thorough investigation into the effect of these parameters should follow, since it can not be known whether similar defaults should be used in conformance checking  as in classical NLP problems.

Apart from that two of the metrics proposed -- WMD and ICT -- do not take the order of words/activities into account.
This is due to the fact that for a lot of NLP problems order is of lesser importance and tasks like document keyword detection can treat a document as an orderless bag of words.
At least from an intuitive understanding of conformance checking order of activities is integrally important, and it should at least be investigated if there are significant differences between ordered and orderless approaches with regard to conformance checking.
The authors themselves suggest considering the use of $n$-grams instead of 1-grams, since their application seems more feasible in business processes as compared to classical NLP.

Lastly the paper should be understood as a first stepping stone for the use of NLP-related algorithms for the task of conformance checking.
The use of embeddings is often a first step in such approaches, but it is entirely possible that more advanced NLP algorithms may give even better results.
As such further research into the applicability of a variety of other methods could be fruitful.
The authors especially mention $n$-grams, Embeddings from Language Models, Global Vectors for Word Representation and recurrent neural networks.

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{template/splncs04}
\bibliography{literature/bibliography}
%
\end{document}
