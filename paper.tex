% !TeX spellcheck = en_GB
% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{template/llncs}
%
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage[autostyle]{csquotes}  
\usepackage{hyperref}
\usepackage{cleveref}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Conformance Checking Using Embeddings and their Applicability in the Internet of Things}
%
\titlerunning{Conformance Checking Using Embeddings in the IoT}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Jan Kruska}
%
\authorrunning{Jan Kruska}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Chair of Process and Data Science\\Department of Computer Science, RWTH Aachen, Aachen, Germany}
%	\email{jan.kruska@rwth-aachen.de}\\
%	\url{http://www.pads.rwth-aachen.de/}}
%
\maketitle              % typeset the header of the contribution
%
%
%
%

\begin{abstract}
	In the last decade there has been growing interest, both scientific and non-scientific, in the field of process mining.
	While the main focus has been on business processes, the methods developed in the field of process mining are in now way limited to that area.
	One other area which lends itself to process mining is the growing Internet of Things.
	While there are many conceivable applications,  there has only been limited research interest in this synergy.
	The aim of this seminar paper is to recapitulate the findings of the focus paper \cite{PBWe20}, while also emphasizing the benefits and difficulties of applying the described methodology in the Internet of Things.
\end{abstract}

\section{Introduction}

Due to growing compute capabilities, as well as further digitalization allowed for more process data to be gathered and analysed.
The increase in compute and storage capabilities, as well as the growing digitalization of many parts of our lives has created the distinct field of data science, as an amalgamation of Statistics, technical knowledge and domain knowledge.

One benefactor of this development has been the subfield of process science, which is concerned with the analysis of processes.
The combination of process science and data science gave rise to techniques generally grouped under the term process mining\footnote{Please note that due to the relative young age of the field, the fact that it lies at the intersection of two different fields and the fact that there is a marketing interest for these topics the terminology is not always settled and different authors and/or groups may differ in usage. The terminology used here is oriented around the definitions in \cite{Aals16}, as it seems to be the most polished overview of the field at the moment.}.
Process mining is concerned with the automatic and algorithmic processing of event data. In its easiest form an event is a piece of data, that consists of a timestamp, an identifier to group events belonging to the same execution and a body containing further arbitrary values.
In most cases there is also an identifier for the event type, with the purpose of identifying similar events in different executions of the process, e.g. to be able to group all "Send invoice" events as the same type.

Historically most research in the topic of process mining has been on business processes. 
There are a few reasons for that. 
Namely, that there was already a well established field of non-algorithmic process science for business processes, which could act as a stepping stone.
Secondly some of these processes were already highly digitized, many companies already had well established ERP systems from which data could be gathered with reasonable ease.
Thirdly there was, and is, a large financial interest in optimizing such processes.
And lastly business processes often struck the balance between too simple and too complicated, in that they exhibited enough variability to be interesting to analyse but were still simple enough as to enable the use of the early process mining algorithms.
So it should be stated that while process mining is of great use for classical business processes, these are not the only areas for which it could potentially be useful.

In the last two decades the cost for networking components has decreases significantly and a growing number of microcontrollers, embedded systems and electronic devices now have some form of networking ability.
This increasing networked structure that enables easy information sharing between the members of the network has become known under a few names, most prominently as the "Internet of Things".
Unfortunately due to shifting developments in the last two decades and marketing efforts associated with the Internet of Things it is hard to pin down the exact meaning of the term. 
The International Telecommunication Union gives the following succinct definition of the Internet of Things in their recommendation \cite{ITUT12} on the topic:
\blockquote{[The Internet of Things is] A  global  infrastructure  for  the  information  society,  enabling  advanced  services  by  interconnecting  (physical  and  virtual)  things  based  on  existing  and  evolving  interoperable information and communication technologies.}

It should be noted that this definition is very broad and extends beyond the notion of the "Smart Home" that has become somewhat of a representative of the Internet of Things.
The definition neither limits the scale (locally, nationally or even globally) nor the range of use cases.
The rest of the paper will follow this definition since the object of interest here are fully digitized networked control systems and the information pooling and sharing those bring with it.


This vast infrastructure of small networked devices could prove to be fertile ground for process analysis.
These controllers generally have some attached sensors to monitor and control the devices they are attached to and in combination with these produce an equally vast amount of events.
This is especially interesting in the case of automation in manufacturing\cite{JKM*20} for which the term Internet of Production has been coined, here the hope is that networked information sharing between controllers on a production line or factory floor could greatly enhance the production capabilities of not just single production lines but entire automated factories.
Especially in the case of manufacturing there is a large financial interest in optimizing production efficiency for various reasons such as increasing speed, increasing productivity or reducing downtime.

However, the Internet of Things is also very attractive from the process analysis research perspective.
During the birth of process mining as a discipline a big focus laid on classical business processes, such as purchase-to-pay and order-to-cash.
Yet as the field matures the capabilities of algorithms and accordingly the difficulty of interesting problems grows.
As such there is research demand \cite{JKM*20} for different -- to test whether algorithms perform equally well in other scenarios --, larger -- to improve scalability of algorithms -- and more complex -- to test and improve the capacity to deal with more interwoven processes -- problems.
At the same time only areas with a high level of digitalization can realistically be considered here since it is of paramount priority to be able to properly obtain the event data of any process in question.
This has been a problem in the past when considering any process for which event data has to be manually entered, since in all these cases there will be discrepancies between digitally recorded process and the real world execution.
The Internet of Things fulfils all these categories, it is digital by design, so relatively few efforts have to be made to obtain large amount of well-enough structured event data and the scale and complexity of the problem can scale up very well with growing network sizes. 
Furthermore, it is also possible to use techniques from the Internet of Things to further digitize classical processes and reduce dependencies on manual event recording.

Naturally there are challenges to be overcome if one aims to integrate business process modelling into the Internet of Things.
The first two major difference are attributable to the structure of the Internet of Things.
Firstly the Internet of Things is made up numerous small devices all with limited compute capabilities.
This limits the complexity of algorithms that can be performed on each of the devices and means that either a separate system with the role of performing analyses needs to be introduced or distributed algorithms need to be developed.
Overall it should be noted that asynchronous decentralized networks are common for the Internet of Things, which needs to be taken into account when applying BPM.
Secondly the Internet of Things has both the potential and demand for greater number of online analyses, i.e. in contrast to classical business processes it is much more important to evaluate a process during the execution, to give feedback and to possibly even influence a process during its execution.

\section{Background}
\subsection{Conformance Checking}
Conformance checking covers a range of different methods that aim to quantify how well an event log (or even just a single trace) and a process model fit together.
Our intuitive expectation would be that a model that allows for exactly those traces contained in the event log would conform perfectly, whereas a model that allows for none of the traces seen in the log would have very low conformance.
The aim of conformance checking is the formalization of this imprecise intuitive understanding of fitting together and the development of methods and algorithms that can accurately quantify precisely this intuition.

\subsubsection{Uses}
The uses of such an approach can generally be divided into two relatively distinct subcategories, depending on whether the event log or the process model are seen as normative.
If the process model is considered normative, conformance checking allows for the detection of deviations, i.e. observed traces that are not possible according to the model.
From such information a number of useful analyses can follow, e.g. where do deviations happen, which resources are involved in deviations, and how exactly these traces deviate from the model.
It should be pointed out that the word "deviation" is negatively connotated in English, however for the purpose of process intelligence it should be viewed as a neutral category. 
For example an analysis may reveal that people often skip a step in the process, which actually improves the overall performance of the process, based on which refinements to the process model could be made.
An example of negative conclusions may be the detections of deviations that do not follow proper legal or procedural requirements of the process, such as requiring two different persons to review an important document before signing off on it. In such a case knowledge of these deviations is required to ensure proper execution of the process.

In the second case the log is seen as normative and the model considered either imperfect or unknown.
The most important representative application of this perspective is process discovery, or more specifically the evaluation of discovery algorithms.
In the case of process discovery a process model has to be constructed from an event log.
Obviously not all process models are able to describe a process equally good.
So when developing or using a process discovery algorithm the question of how well the discovered process models describe the log often arises.

\subsubsection{Quality criteria}
It is important to note that model quality is not a single dimensional criterion, instead it exists in a multi-dimensional space where the different axes represent different quality criteria.
As such it is not really appropriate to talk about one conformance metric, but instead the four major axes should be considered.
1. Fitness (few false negatives): The first of these metrics is fitness. This quality criterion is closely related to the statistical measure of recall. In the case of business process management fitness measures how many of the observed traces are actually possible executions of the model.
2. Precision (few false positives): Closely related to the statistical measure of precision, this metric measures how many false positives there are in relation to true positives. In the case of process models, this is the fraction of traces allowed by the model that are actually observed in the log.
3. Simplicity: How "simple" a model is, generally a smaller model that describes a process equally well is preferred for reasons such as performance and comprehensibility .
4. Generalization: Generalization is the model's ability to be applicable to possible future traces of a given process. 
It would clearly be possible to create a model with perfect precision and recall, by simply encoding the traces themselves in the model. However, that would not be a model that generalizes well since it captures none of the dynamism of the underlying process. 
This is the most difficult criterion, since it is arguing about unknown possible future behaviour.

Of these four quality criteria conformance checking is mainly concerned precision and recall, but depending on the algorithms possibly some statements about generalization could be made as well. 

\subsubsection{Token based Replay}

The easiest conformance checking method, both conceptually and algorithmically is probably Token based replay.
The intuition behind token based replay is to replay an observed trace on a process model, specifically a Petri net, and to determine where problems occur.

This is done by keeping track of four numbers: \emph{\underline{p}roduced}, \emph{\underline{c}onsumed}, \emph{\underline{m}issing} and \emph{\underline{r}emaining} tokens during replay.
According to the semantics of Petri nets, whenever a transition is fired it consumes one token from all input places and produces one token in each output place.
However since we are dealing with possibly non-conformant trace, it is possible that a trace contains activities, that are not enabled in the model at that point in time.
In this case a missing token is added to all input places, that did not contain a token, and the $m$ counter is incremented by the according amount.
Conversely, it might also happen, that a trace can be replayed, but that there are stray tokens left in the Petri net after the replay. 
At the end of the replay the sum of all leftover tokens makes up the $r$ counter. \todo{Some explanation models could be included if the space is there for them}
Both missing and remaining tokens signal deviation from the process model, but both of these numbers are dependent on the length of traces.
The longer a trace is the more activities will be taken, the more tokens will be produced, and the more chances there are for tokens to be missing or remaining.
Importantly a trace where half the produced tokens were never consumed, i.e. remained, should intuitively have a lower fitness than one where only a quarter of produced tokens remained.
Of course the same relationship holds for the other two counters $m$ and $c$.
Then one very naturally arrives at a trace fitness defined as:
\begin{equation}
fitness(\sigma,N) = \frac{1}{2}(1-\frac{m_{N,\sigma}}{c_{N,\sigma}})+\frac{1}{2}(1-\frac{r_{N,\sigma}}{p_{N,\sigma}})
\end{equation}
, where $x_{N,\sigma}$ is the counter when replaying trace $\sigma$ on Petri net $N$.
Often one does not just want to determine the fitness of one trace but a complete log, then this definition can be extended to 
\begin{equation}
	fitness(L,N) = \frac{1}{2}(1-\frac{\sum_{\sigma\in L}m_{N,\sigma}}{\sum_{\sigma\in L}c_{N,\sigma}})+\frac{1}{2}(1-\frac{\sum_{\sigma\in L}r_{N,\sigma}}{\sum_{\sigma\in L}p_{N,\sigma}})
\end{equation}
Note that log-fitness is not mathematically equivalent to the mean trace fitness.

Token based methods are generally easy to implement and efficient algorithms are known, however they are not without problems.
The first is the phenomenon known as "token flooding". Since token based methods create missing tokens to enable observed transitions in the log and remaining tokens are not removed, as more missing tokens are added during the simulation of a trace the  amount of stray tokens in the net also grows.
This overabundance of tokens in the net can then lead to situations were many more transitions are enabled than should be, and later deviations are not detected by token based replay, since they are masked by earlier deviations.
Also, token based diagnostics are  by their nature place-based diagnostics, they can determine whether a place was missing a token for the execution of a trace, but they cannot determine why said place was missing a token. 
Yet in conformance checking it is often the latter rather than the former question for which answers are desired. 

\subsubsection{Alignments}

The second popular method for conformance checking are alignment based methods.
The goal here is to construct an execution on the model that is as "close" as possible to the observed execution of events in the trace/log.
If a trace is conformant with the model, this is trivially easy, the "closest" execution possible in the model is exactly the observed trace.
In the conformant case all moves are so called, synchronous moves, where both the log and the model performed the same event at each step.
However, if a trace is non-conformant, which is of course the case that is of interest, then a one-to-one mapping between model and trace is not possible.
To describe that so-called non-synchronous moves are introduced, these describe the case that one of  the model or log perform an activity that does not happen in the other one.
This can be either a move-on-model-only, where the model takes some transition, which was not observed in the log, or its dual, the move-on-log-only where an event is recorded in the log that is not possible according to the model.
The weighted sum of misalignments can then be used as a cost metric for an alignment. 
The goal is then to construct an optimal alignment between a trace and the model.
It should be obvious that there are infinitely many such alignments, however we can construct a trace-agnostic alignment, which consists of $n$ moves-on-log-only followed by the shortest path through the model as moves-on-model-only.
This means there is an effective upper limit on the length of alignments that need to be considered, which in turn means that the problem is decidable. 
However, the search space can quickly grow infeasibly large, so heuristics need to be employed to determine optimal alignments in a reasonable time frame.

\todo{formula}
\todo{Possibly some examples}

Alignments overcome many of the problems encountered in token based approaches: 
They do not suffer from problems like token flooding, and they give transitions based diagnostics, which in the case of process mining is far more useful since transitions encode recorded or recordable activities. 
As such in alignments it is much easier to tell if a certain activity was skipped in a trace without performing further refinements of the diagnostics.
These benefits do not come without drawbacks though, the main caveat of alignment based methods is that it boils down to a complex search in the alignment space, which is far more demanding than a replay based method.
As such much research interest has been devoted to the development of heuristics that exploit certain  structures in event logs and/or process models to boost the performance of this search.

\subsection{Natural Language Processing}

As laid out in the previous section, both classical branches of conformance checking exhibit major drawbacks.
Besides approaches aiming to ameliorate certain disadvantages of such methods, there has also been a growing interest in applying novel techniques from other fields to the problem of conformance checking.
One such proposal is the application of techniques from the area of natural language processing for conformance checking as discussed in \cite{PBWe20}.

\subsection{Embeddings}
Embeddings are a popular machine learning approach that has been used for a variety of tasks with the most successful applications being in the fields of natural language processing and computer vision.
The fundamental idea of embeddings is to train some form of embedding function that embeds complex objects, such as words or documents in the case of NLP or pictures and video sequences in the case of computer vision in a vectors space\footnote{To be more precise: a Hilbert space, since well-defined notions of distances and angles are necessary and for that a scalar product  is needed.}.
Such an embedding function then enables a proper mathematical treatment of these non-vector objects as vectors, which allows for the application of a broad range of algorithms.
Most importantly it makes it possible to define notions of distance and/or (dis-)similarity needed for machine learning and data analysis tasks.

\subsection{word2vec}

\section{Method}

Many conformance checking algorithms, notably the classical ones outlined above, aim to measure conformance between a log and a model.
While this is the fundamental question to be answered, this comparison between two different types of objects greatly complicates the task.
However, by re-framing the problem on can greatly simplify it.
Instead of formulating the problem in terms of model-to-log comparison, one can formulate it as a comparison problem of a real log to a model log.
Such a reformulation of the problem is possible since all (reasonably defined) models are able to generate sample traces, and can therefore produce a model log.
Then the whole problem is reframed as log-to-log comparisons, i.e. determining similarities between objects of the same type, for which a broad range of techniques already exist.
This has the additional advantage that it is model-independent, since no limitations apart from the ability to generate traces are put on the type of model.
\begin{figure}
	\includegraphics[width=1\textwidth]{figures/structure}
	\caption{General structure of the approach}
	\label{fig:structure}
\end{figure}

\color{blue}
Three different techniques were outlined in the paper, all following a similar structure outlined in \cref{fig:structure}.
First a model log is produced from the model by using an appropriate replay semantics for the model.
Then, given these two logs, one real and one produced by replaying the model, it is possible to train the \emph{act2vec}\cite{KBWe18} embedding.
Based on these embeddings a dissimilarity function needs to be defined to compare different traces, however the \emph{act2vec} embedding only yields distance measurements between activities and not between traces.
Here the differences between the three techniques come into play.
The first two, the Word Mover's Distance and Iterative Constrained Transfers, can directly utilize the distances between activities to determine a distance between two collections of activities.
The last one, \emph{trace2vec}, further needs to train a trace embedding based on activity embeddings, and can then perform direct comparisons in the trace embedding space.
All three methods however lead to a reasonable dissimilarity metric between traces.
Using one of these metrics a dissimilarity matrix can be built up describing the pairwise dissimilarities between the real log and the model log.
\color{black}
From here on we will assume that each row corresponds to a trace from the model log and each column corresponds to a trace from the real log, i.e. $a_{ij}$ is the dissimilarity of the $i$-th trace from the model log with the $j$-th trace of the real log.
As such each column contains the dissimilarity of a trace in the real log with all traces from the model log, with the minimum of a column being the closest match in the model log.
A trace that can be closely replayed by the model, i.e. one that has a close match to a trace produced from the model, is a well fitting one.
As such the column minimum corresponds to a fitness metric of the trace and thus the mean of all column minima is a fitness metric between log and log.
Conversely, each row contains all similarities between a trace in the model log and all traces in the real log.
A precise model should not allow for more traces than observed in the real log, as such each model trace should have as close a match as possible in the real log for the model to be precise.
As such the row minimum corresponds to a precision metric for the trace in the model log and thus the mean of all row minima is a precision metric between log and log.

\subsection{act2vec}
The first two methods utilize \emph{act2vec} to train activity embeddings.
These activity embeddings then allows for the comparison of two traces by comparing their individual activities in the embedding space.
Then every trace constitutes a set of different activities that can be expressed in the embedding space.
Every trace then describes a probability distribution over the activities contained in it, so given an appropriate distance metric for probability distributions such as the Earth Movers Distance it is possible to determine the distance (or conversely similarity) of two traces.

\subsubsection{Word Mover's Distance}

\todo{Use pseudocode from paper?}
\todo{Describe both WMD and ICT in separate sections, or just mention that ICT gives a relaxed formulation of WMD that can be easier to compute}

\subsection{trace2vec}
In contrast to the two previous approaches, which used activity embeddings, it is also possible to train trace embeddings.
The relationship between \emph{act2vec} and \emph{trace2vec} is analogous to the one between \emph{word2vec} and \emph{doc2vec}.
When using this method every trace is embedded into a vector space and any distance metric that is deemed appropriate can be chosen.
In the paper the cosine similarity \cref{eq:cosine} was chosen for its simplicity and the fact that it is bounded in the interval $[-1,1]$.
\begin{equation}
	\cos(x,  y) = \frac { x \cdot  y}{|| x|| \cdot || y||}
	\label{eq:cosine}
\end{equation}
This is much less computationally expensive as the previous two approaches which were complex distance metric between two sets of embeddings.

\subsection{Definition of Fitness and precision}


\section{Results}

\subsection{Experimental setup}

To evaluate the suitability of the method three sets of experiments were performed. 
\color{blue}
All experiments utilized (the same) randomly generated process trees, that were constructed using the algorithms in the \emph{pm4py} library.
Three size configurations described by minimum number, the mode and the maximum number of visible activities were chosen: 5-10-15, 10-20-30 and 15-30-45.
Furthermore, four operator configurations were chosen described by the probability for randomly choosing each available operator.
These were in no particular order: Sequence: $0.75$, Parallel $0.25$, secondly Sequence: $0.75$, Choice $0.25$, thirdly 1. Sequence: $0.5$, Parallel $0.25$, Choice $0.25$ and lastly $0.25$ for all (Sequence, Parallel, Choice and Loop) operators.
Both of these configuration were combined to generate a grid of $12$ process trees, with the hope that these cover a sufficient range of different behaviour.
\color{black}

The first set of experiments focussed on the scalability of the approach, especially since more traditional approaches do not scale well. This aspect is doubly important when talking about the Internet of Things, where computing resources are generally far more constrained, while still producing large amounts of event data.

The second set of experiments aimed to determine whether the method responded as expected when providing noisy logs.
These experiments were designed to verify the soundness of approach.
This was done by generating logs from random process trees generated by pm4py, applying random noise on these logs and determining the conformance between the noiseless and noisy logs.
The expectation is that any reasonable conformance metric should deteriorate the more noise is added to the log.

The third and last set of experiments was designed to compare the performance of the proposed methods (WMD,ICT \& trace2vec) to existing established conformance metrics, namely alignments, behavioural fitness \cite{GMVB09} and ETC precision \cite{MuCa10}.
This was achieved by generating 1000 traces from a randomly generated process tree, which form the real log.
Then a discovery algorithm was applied to this log to discover a model, here Alpha Miner, Inductive Miner infrequent (IMi) with noise levels 0 and 1 and the ILP Miner were selected as discovery algorithms.
Then fitness and precision between the discovered model and the real log were calculated using the previously mentioned algorithms as well as the method described in the paper.

\color{blue}
\subsection{Scalability experiment}
\begin{figure}
	\includegraphics[width=1\textwidth]{figures/scaling}
	\caption{Runtimes for all three methods for varying sizes of logs and dictionaries. Data can be found in \cite{MuCa10} Table 2}
	\label{fig:scalability}
\end{figure}
The first performed experiment aimed to asses the performance  of the three proposed methods.
The experiments were run on an Intel Core i7-9850h, a mid-range consumer CPU.
Experiments were run for all three methods for different sized logs and different sized dictionaries, in this case dictionary size is determined by the number of different activities in the log.
All experiments were performed in python, however the Word Mover's distance was calculated using pyemd, a python wrapper to calculate the WMD/Emd with numpy efficiency.
As such when comparing ICT and WMD times the efficiency difference between python and C should be taken into account.

The resulting times are shown in \cref{fig:scalability}.
Over all log sizes a similar pattern is observable, with WMD and ICT behaving somewhat similar and \emph{trace2vec} consistently outperforming the other two methods in all experiments.
For small dictionary sizes  (~10) WMD does outperform ICT, however that is likely due to the aforementioned efficiency differences between the implementations.
It is doubly notable that despite these efficiency differences ICT already starts to outperform WMD for dictionary sizes of 20, with the gap growing even larger for dictionaries of size 30, ICT is finished after roughly half the time WMD takes.
These experiments seem to suggest that ICT scales much better with larger dictionary sizes.
Especially when considering that the current implementation is still unoptimized ICT could very well serve as a more efficient alternative to WMD, especially when working with complex logs.
It should also be noted that the relative run time difference between both methods seems so slightly scale in the ICT's favor, starting out at $\frac{45}{75}=0.6$ for a log of size 500 and improving to $\frac{3824}{7032}\approx 0.54$ for logs of size 5000.

Lastly the \emph{trace2vec} methods substantially outperforms both the other methods, since only the cosine similarity, a simple mathematical formula needs to evaluated at run time.
This comes with the additional overhead of training trace embeddings, however according to the authors training that "does not take a long time" but they don't include statistics in that regard.
The \emph{trace2vec} method has the additional benefit of not being dependent on the activities during run time, since traces are directly compared in the trace embedding space, which means that the size of the dictionary is irrelevant for this approach, making it even more attractive for processes with large numbers of activities than the ICT.
\todo{Include log size scaling plot? Do I have the space for that? It is not really that interesting as far as I can tell}

\subsection{Noise experiment}



\subsection{Discovery experiment}


The purpose of the last experiment was the comparison of the three proposed techniques to established conformance checking algorithms.
The results of this experiment can be found in \cite{PBWe20} Table 5.
\todo{Include their data? Manually as table or maybe as plot. If so what kind plot would you suggest}
The results from the discovery experiment seem to suggest that all three methods are able to capture a notion of conformance.
Especially when the model is of high quality both the proposed and validation conformance checking techniques often agree on a perfect score.
However, it should be noted that there are quite a few instances where all three proposed methods assign perfect fitness or precision despite the others producing imperfect scores.
This is worrying since one of the most basic requirements for a good conformance checking technique is not assigning perfect scores to models that are able to replay fewer or more traces than in the log.
It is unclear whether that is due to the unoptimized nature of the methods, it could be the case that the methods are not sensitive enough, which could be fixed by varying the parameters, or due to the limited size of the model logs, which were limited to 1000 traces, which puts an upper limit on the amount of behaviour that the embeddings are able to observe.

Yet when models deteriorate there is a tendency for all measurements to move together.
At the current point only a qualitative analysis for imperfect models is possible since the values determined by WMD, ICT and \emph{trace2vec} are not proper metrics since they do not fall into the $0,1$ range.
As such it is hard to interpret what a non optimal value actually means, and as such the data does not allow for a quantitative comparison  at this point.
The results that were obtained however suggest that these methods could be a useful alternative to established techniques, and that further research into them is warranted.

It is of course also important to point out that the validation techniques should not be seen as perfect normative techniques, since both of these also significantly disagree for some models. 

\color{black}
\section{Discussion}

\todo{Proper fitness metric}

Peeperkorn et al. obtained promising first results for a novel bedding based approach towards conformance checking.
Yet as the focus was mainly on the introduction and development of a prototype for such an approach there are some natural limitations.
Arguably the biggest limitation are the logs used for the experimental validation.
All used logs were completely synthetic, generated from random process trees.
It is unclear how well the 12 trees used for this resemble real processes and whether these tree configurations are sufficiently representative for the variation exhibited in real life trees\footnote{12 tree configurations were used, 3 sizes and 4 different probability distributions for operations, how strong these vary should be investigated}.
As such it should be a priority to test the proposed methods on real logs, possibly popular benchmark cases and also to systematically broaden the parameters for random process tree generation to cover more different classes.

\color{blue}
It is also a  prerequisite for further research and possible usage of such methods to convert the distance metrics obtained into proper conformance metrics, especially by restricting values to the interval $[0,1]$.
When that is done further quantitative comparisons between the updated methods and established conformance checking techniques should be performed.
Furthermore, the algorithms used have several parameters which were set as seemingly reasonable standard values.
A thorough investigation into the effect of these parameters should follow, since it can not be known whether similar defaults should be used in conformance checking  as in classical NLP problems.
Special attention should be placed on those cases for which the NLP techniques produced perfect values, whereas the others did not.
The methods should be tested with completely perfect and completely imperfect models regarding fitness and precision, as well as nearly perfect and nearly imperfect models to verify that the methods are able to correctly classify both ends of model spectrum and not misclassify nearly optimal models as perfect.
\color{black}
Apart from that two of the metrics proposed -- WMD and ICT -- do not take the order of words/activities into account.
This is due to the fact that for a lot of NLP problems order is of lesser importance and tasks like document keyword detection can treat a document as an orderless bag of words.
At least from an intuitive understanding of conformance checking order of activities is integrally important, and it should at least be investigated if there are significant differences between ordered and orderless approaches with regard to conformance checking.
The authors themselves suggest considering the use of $n$-grams instead of 1-grams, since their application seems more feasible in business processes as compared to classical NLP.

Lastly the paper should be understood as a first stepping stone for the use of NLP-related algorithms for the task of conformance checking.
The use of embeddings is often a first step in such approaches, but it is entirely possible that more advanced NLP algorithms may give even better results.
As such further research into the applicability of a variety of other methods could be fruitful.
The authors especially mention $n$-grams, Embeddings from Language Models, Global Vectors for Word Representation and recurrent neural networks.

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{template/splncs04}
\bibliography{literature/bibliography}
%
\end{document}
