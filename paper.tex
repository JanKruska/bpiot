% !TeX spellcheck = en_GB
% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{template/llncs}
%
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{textalpha}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Seminar Paper}
\subtitle{Seminar Business Processes and the Internet of Things \\ Conformance Checking Using Activity and Trace Embeddings.}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Jan Kruska \\ Supervisor: Dr. István Koren}
%
\authorrunning{Jan Kruska}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Chair of Process and Data Science\\Department of Computer Science, RWTH Aachen, Aachen, Germany}
%	\email{jan.kruska@rwth-aachen.de}\\
%	\url{http://www.pads.rwth-aachen.de/}}
%
\maketitle              % typeset the header of the contribution
%
%
%
%
%\section{Planned structure}
%The following list shall give a short overview of the intended general structure of the seminar paper. For each section the main questions that should be answered by it are listed.
%\begin{enumerate}
%	\item Abstract
%	\item Introduction
%	\begin{enumerate}
%		\item What is the motivation of the paper
%	\end{enumerate}
%	\item Background
%	\begin{enumerate}
%		\item What is conformance checking, how is it useful and why would you do it, which methods are there, what are advantages and disadvantages of said methods.
%		\item What is NLP (briefly), what are embeddings generally and word embeddings specifically, what are word2vec and doc2vec and how do they approach the NLP problems.
%		\item What parallels are there between NLP and conformance checking
%		\item If there are these parallels, how could such NLP mechanism be adapted (theoretically not practically) for conformance checking, what could the advantages be of such an approach, what could be disadvantages and/or problems that will need addressing.
%		\item What is the Internet of Things and why would one wish to perform process analysis there
%	\end{enumerate}
%	\item Method
%	\begin{enumerate}
%		\item How were word2vec and doc2vec adapted to conformance checking in the paper (I.e. practical adaption)
%	\end{enumerate}
%	\item Results
%	\begin{enumerate}
%		\item What are the results given in the paper
%		\item What metrics were used to evaluate the performance of the approach presented in the paper, are they appropriate, are there other possible metrics, if so why was this one chosen, what advantages or disadvantages do different metrics have.
%		\item What are the results when using their code and reproducing the results (ideally the same)
%	\end{enumerate}
%	\item Outlook
%	\begin{enumerate}
%		\item What future work do the authors suggest
%		\item What problems were encountered when trying to reproduce the results from the paper. Could they be reproduced completely, just in part or not at all? If so, are there suggestions on how could these problems be mitigated.
%	\end{enumerate}
%\end{enumerate}

\begin{abstract}
	In the last decade there has been growing interest, both scientific and non-scientific, in the field of process mining.
	While the main focus has been on business processes, the methods developed in the field of process mining are in now way limited to that area.
	One other area which lends itself to process mining is the growing Internet of Things.
	While there are many conceivable applications,  there has only been limited research interest in this synergy.
	The aim of this seminar paper is to recapitulate the findings of the focus paper \cite{PBWe20}, while also emphasizing the benefits and difficulties of applying the described methodology in the Internet of Things.
\end{abstract}

\section{Introduction}

Due to growing compute capabilities, as well as further digitalization allowed for more process data to be gathered and analysed.
The increase in compute and storage capabilities, as well as the growing digitalization of many parts of our lives has created the distinct field of data science, as an amalgamation of Statistics, technical knowledge and domain knowledge.

One benefactor of this development has been the subfield of process science, which is concerned with the analysis of processes.
The combination of process science and data science gave rise to techniques generally grouped under the term process mining\footnote{Please note that due to the relative young age of the field, the fact that it lies at the intersection of two different fields and the fact that there is a marketing interest for these topics the terminology is not always settled and different authors and/or groups may differ in usage. The terminology used here is oriented around the definitions in \cite{Aals16}, as it seems to be the most polished overview of the field at the moment.}.
Process mining is concerned with the automatic and algorithmic processing of event data. In its easiest form an event is a piece of data, that consists of a timestamp, an identifier to group events belonging to the same execution and a body containing further arbitrary values.
In most cases there is also an identifier for the event type, with the purpose of identifying similar events in different executions of the process, e.g. to be able to group all "Send invoice" events as the same type.

Historically most research in the topic of process mining has been on business processes. 
There are a few reasons for that. 
Namely, that there was already a well established field of non-algorithmic process science for business processes, which could act as a stepping stone.
Secondly some of these processes were already highly digitized, many companies already had well established ERP systems from which data could be gathered with reasonable ease.
Thirdly there was, and is, a large financial interest in optimizing such processes.
And lastly business processes often struck the balance between too simple and too complicated, in that they exhibited enough variability to be interesting to analyse but were still simple enough as to enable the use of the early process mining algorithms.
So it should be stated that while process mining is of great use for classical business processes, these are not the only areas for which it could be useful.

One such area of interest is the Internet of Things.
In recent years a vast infrastructure of small networked devices typically with some sensors attached has developed. 
This is especially interesting in the case for automation in manufacturing for which the term Internet of Production has been coined.
Both these "Internet"'s\footnote{Note that there will not be a very clear distinction between these two terms in this paper. The object of interest here is a highly distributed network of interacting devices, that generate a large amount of event data about their surroundings. Which specific environment they are in is less important here.} 
constitute a highly digitized network in which avast amount of data, including event data, is being generated and measured.
Especially in the case of manufacturing there is a large monetary interest in optimizing production pipelines.
And lastly in the case of the Internet of Things there are complex hidden processes to be discovered, which can challenge newer and more advanced process mining algorithms in a way that other areas might not.
\section{Background}
\subsection{Conformance Checking}
Conformance checking covers a range of different methods that aim to quantify how well an event log (or even just a single trace) and a process model fit together.
Our intuitive expectation would be that a model that allows for exactly those traces contained in the event log would conform perfectly, whereas a model that allows for none of the traces seen in the log would have very low conformance.
The aim of conformance checking is the formalization of this imprecise intuitive understanding of fitting together and the development of methods and algorithms that can accurately quantify precisely this intuition.

\subsubsection{Uses}
The uses of such an approach can generally be divided into two relatively distinct subcategories, depending on whether the event log or the process model are seen as normative.
If the process model is considered normative, conformance checking allows for the detection of deviations, i.e. observed traces that are not possible according to the model.
From such information a number of useful analyses can follow, e.g. where do deviations happen, which resources are involved in deviations, and how exactly these traces deviate from the model.
It should be pointed out that the word "deviation" is negatively connotated in English, however for the purpose of process intelligence it should be viewed as a neutral category. 
For example an analysis may reveal that people often skip a step in the process, which actually improves the overall performance of the process, based on which refinements to the process model could be made.
An example of negative conclusions may be the detections of deviations that do not follow proper legal or procedural requirements of the process, such as requiring two different persons to review an important document before signing off on it. In such a case knowledge of these deviations is required to ensure proper execution of the process.

In the second case the log is seen as normative and the model considered either imperfect or unknown.
The most important representative application of this perspective is process discovery, or more specifically the evaluation of discovery algorithms.
In the case of process discovery a process model has to be constructed from an event log.
Obviously not all process models are able to describe a process equally good.
So when developing or using a process discovery algorithm the question how well the discovered process models describe the log often arises.

\subsubsection{Quality criteria}
%Generally conformance is not a single dimensional criterion, instead it exists in a multi-dimensional space where the different axes represent different understandings of conformance.
As such it is not really appropriate to talk about one conformance metric, but instead the four major axes should be considered.
%1. Precision (false negatives): The first of these metrics is precision. This measures how many of the observed traces are actually possible according to the model.
%2. Recall (false positives): Sometimes also referred to as sensitivity, this metric measures how many false positives there are. In the case of process models, this is the fraction of traces allowed by the model that are actually observed in the log.
%3. Simplicity:
%4. Generalization: Generalization is the models ability to be applicable to possible future traces of a given process. 
%It would clearly be possible to create a model with perfect precision and recall, by simply encoding the traces themselves in the model. However that would not be a model that generalizes well since it captures none of the dynamism of the underlying process. 
This is the most difficult criterion, since it is arguing about unknown possible future behaviour

\subsubsection{Token based Replay}

The easiest conformance checking method, both conceptually and algorithmically is probably Token based replay.
The intuition behind token based replay is to replay an observed trace on a process model, specifically a petri net, and to determine where problems occur.

This is done by keeping track of four numbers: \emph{\underline{p}roduced}, \emph{\underline{c}onsumed}, \emph{\underline{m}issing} and \emph{\underline{r}emaining} tokens during replay.
According to the semantics of petri nets, whenever a transition is fired it consumes one token from all input places and produces one token in each output place.
However since we are dealing with possibly non-conformant trace, it is possible that a trace contains activities, that are not enabled in the model at that point in time.
In this case a missing token is added to all input places, that did not contain a token, and the $m$ counter is incremented by the according amount.
Conversely, it might also happen, that a trace can be replayed, but that there are stray tokens left in the petri net after the replay. 
At the end of the replay the sum of all leftover tokens makes up the $r$ counter. \todo{Some explanation models could be included if the space is there for them}
Both missing and remaining tokens signal deviation from the process model, but both of these numbers are dependent on the length of traces.
The longer a trace is the more activities will be taken, the more tokens will be produced, and the more chances there are for tokens to be missing or remaining.
Importantly a trace where half the produced tokens were never consumed, i.e. remained, should intuitively have a lower fitness than one where only a quarter of produced tokens remained.
Of course the same relationship holds for the other two counters $m$ and $c$.
Then one very naturally arrives at a trace fitness defined as:
\begin{equation}
fitness(σ,N) = \frac{1}{2}(1-\frac{m_{N,σ}}{c_{N,σ}})+\frac{1}{2}(1-\frac{r_{N,σ}}{p_{N,σ}})
\end{equation}
, where $x_{N,σ}$ is the counter when replaying trace $σ$ on petri net $N$.
Often one does not just want to determine the fitness of one trace but a complete log, then this definition can be extended to 
\begin{equation}
	fitness(L,N) = \frac{1}{2}(1-\frac{\sum_{σ\in L}m_{N,σ}}{\sum_{σ\in L}c_{N,σ}})+\frac{1}{2}(1-\frac{\sum_{σ\in L}r_{N,σ}}{\sum_{σ\in L}p_{N,σ}})
\end{equation}
Note that log-fitness is not mathematically equivalent to the mean trace fitness.

Token based methods are generally easy to implement and efficient algorithms are known, however they are not without problems.
The first is the phenomenon known as "token flooding". Since token based methods create missing tokens to enable observed transitions in the log and remaining tokens are not removed, as more missing tokens are added during the simulation of a trace the  amount of stray tokens in the net also grows.
This overabundance of tokens in the net can then lead to situations were many more transitions are enabled than should be, and later deviations are not detected by token based replay, since they are masked by earlier deviations.
Also, token based diagnostics are  by their nature place-based diagnostics, they can determine whether a place was missing a token for the execution of a trace, but they cannot determine why said place was missing a token. 
Yet in conformance checking it is often the latter rather than the former question for which answers are desired. 

\subsubsection{Alignments}

The second popular method for conformance checking are alignment based methods.
The goal here is to construct an execution on the model that is as "close" as possible to the observed execution of events in the trace/log.
If a trace is conformant with the model, this is trivially easy, the "closest" execution possible in the model is exactly the observed trace.
In the conformant case all moves are so called, synchronous moves, where both the log and the model performed the same event at each step.
However, if a trace is non-conformant, which is of course the case that is of interest, then a one-to-one mapping between model and trace is not possible.
To describe that so-called non-synchronous moves are introduced, these describe the case that one of  the model or log perform an activity that does not happen in the other one.
This can be either a move-on-model-only, where the model takes some transition, which was not observed in the log, or its dual, the move-on-log-only where an event is recorded in the log that is not possible according to the model.
The weighted sum of misalignments can then be used as a cost metric for an alignment. 
The goal is then to construct an optimal alignment between a trace and the model.
It should be obvious that there are infinitely many such alignments, however we can construct a trace-agnostic alignment, which consists of $n$ moves-on-log-only followed by the shortest path through the model as moves-on-model-only.
This means there is an effective upper limit on the length of alignments that need to be considered, which in turn means that the problem is decidable. 
However, the search space can quickly grow infeasibly large, so heuristics need to be employed to determine optimal alignments in a reasonable time frame.

\todo{formula}
\todo{Possibly some examples}

Alignments overcome many of the problems encountered in token based approaches: 
They do not suffer from problems like token flooding, and they give transitions based diagnostics, which in the case of process mining is far more useful since transitions encode recorded or recordable activities. 
As such in alignments it is much easier to tell if a certain activity was skipped in a trace without performing further refinements of the diagnostics.
These benefits do not come without drawbacks though, the main caveat of alignment based methods is that it boils down to a complex search in the alignment space, which is far more demanding than a replay based method.
As such much research interest has been devoted to the development of heuristics that exploit certain  structures in event logs and/or process models to boost the performance of this search.

\subsection{Natural Language Processing}

As laid out in the previous section, both classical branches of conformance checking exhibit major drawbacks.
Besides approaches aiming to ameliorate certain disadvantages of such methods, there has also been a growing interest in applying novel techniques from other fields to the problem of conformance checking.
One such proposal is the application of techniques from the area of natural language processing for conformance checking as discussed in \cite{PBWe20}.

\section{Method}

A lot of conformance checking algorithms, notably the classical ones outlined above aim to measure conformance between a log and a model.
While this is the fundamental question to be answered this comparison between two different objects greatly complicates the task.
Instead framing the problem in terms of log-to-log comparisons can make the problem much easier to deal with, and allows for a much broader array of techniques, since a multitude of approaches for comparison of similar objects exists.
To realize this change of perspective a log is generated from the model, which is then compared to the original log. 

\section{Results}

\section{Discussion}

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{template/splncs04}
\bibliography{literature/bibliography}
%
\end{document}
